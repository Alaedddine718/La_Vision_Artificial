{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ–¼ï¸ Clasificador CNN para CIFAR-10\n",
        "\n",
        "**Autores:** Alessio Cicilano & Alaeddine Daoudi  \n",
        "**Fecha:** Octubre 2025\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“‹ DescripciÃ³n\n",
        "\n",
        "Este notebook implementa una **Red Neuronal Convolucional (CNN)** para clasificar imÃ¡genes del dataset CIFAR-10 en 10 categorÃ­as:\n",
        "\n",
        "âœˆï¸ AviÃ³n | ğŸš— AutomÃ³vil | ğŸ¦ PÃ¡jaro | ğŸ± Gato | ğŸ¦Œ Ciervo | ğŸ• Perro | ğŸ¸ Rana | ğŸ´ Caballo | â›µ Barco | ğŸšš CamiÃ³n\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ—ï¸ Arquitectura del Modelo\n",
        "\n",
        "- **Conv2D** (32 filtros 3x3) + ReLU + MaxPooling\n",
        "- **Conv2D** (64 filtros 3x3) + ReLU + MaxPooling\n",
        "- **Flatten**\n",
        "- **Dense** (64 neuronas) + ReLU\n",
        "- **Dense** (10 neuronas) + Softmax\n",
        "\n",
        "**Total parÃ¡metros:** ~167.562\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1ï¸âƒ£ ConfiguraciÃ³n e ImportaciÃ³n de LibrerÃ­as\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar librerÃ­as necesarias\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# ConfiguraciÃ³n de estilo\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(f\"âœ… TensorFlow version: {tf.__version__}\")\n",
        "print(f\"âœ… GPU disponible: {tf.config.list_physical_devices('GPU')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2ï¸âƒ£ Carga y Preprocesamiento del Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar dataset CIFAR-10\n",
        "print(\"ğŸ“¥ Cargando dataset CIFAR-10...\")\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "# Nombres de las clases\n",
        "class_names = ['AviÃ³n', 'AutomÃ³vil', 'PÃ¡jaro', 'Gato', 'Ciervo', \n",
        "               'Perro', 'Rana', 'Caballo', 'Barco', 'CamiÃ³n']\n",
        "\n",
        "# InformaciÃ³n del dataset\n",
        "print(f\"\\nğŸ“Š InformaciÃ³n del Dataset:\")\n",
        "print(f\"   - ImÃ¡genes de entrenamiento: {train_images.shape[0]:,}\")\n",
        "print(f\"   - ImÃ¡genes de prueba: {test_images.shape[0]:,}\")\n",
        "print(f\"   - Dimensiones de imagen: {train_images.shape[1:]}\")\n",
        "print(f\"   - NÃºmero de clases: {len(class_names)}\")\n",
        "print(f\"   - Rango de pÃ­xeles: [{train_images.min()}, {train_images.max()}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NormalizaciÃ³n de las imÃ¡genes (0-255 -> 0-1)\n",
        "print(\"ğŸ”§ Normalizando imÃ¡genes...\")\n",
        "train_images_norm = train_images.astype('float32') / 255.0\n",
        "test_images_norm = test_images.astype('float32') / 255.0\n",
        "\n",
        "# Convertir etiquetas a one-hot encoding\n",
        "print(\"ğŸ”§ Codificando etiquetas...\")\n",
        "train_labels_cat = to_categorical(train_labels, 10)\n",
        "test_labels_cat = to_categorical(test_labels, 10)\n",
        "\n",
        "print(f\"\\nâœ… Preprocesamiento completado!\")\n",
        "print(f\"   - Rango normalizado: [{train_images_norm.min()}, {train_images_norm.max()}]\")\n",
        "print(f\"   - Shape etiquetas: {train_labels_cat.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3ï¸âƒ£ VisualizaciÃ³n del Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar 25 imÃ¡genes aleatorias del dataset\n",
        "plt.figure(figsize=(12, 12))\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    idx = np.random.randint(0, len(train_images))\n",
        "    plt.imshow(train_images[idx])\n",
        "    plt.title(f\"{class_names[train_labels[idx][0]]}\", fontsize=10, fontweight='bold')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.suptitle('ğŸ–¼ï¸ Ejemplos del Dataset CIFAR-10', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4ï¸âƒ£ ConstrucciÃ³n del Modelo CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ConstrucciÃ³n de la arquitectura CNN\n",
        "print(\"ğŸ—ï¸ Construyendo modelo CNN...\\n\")\n",
        "\n",
        "model = models.Sequential([\n",
        "    # Primer bloque convolucional\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3), name='conv1'),\n",
        "    layers.MaxPooling2D((2, 2), name='pool1'),\n",
        "    \n",
        "    # Segundo bloque convolucional\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', name='conv2'),\n",
        "    layers.MaxPooling2D((2, 2), name='pool2'),\n",
        "    \n",
        "    # Clasificador\n",
        "    layers.Flatten(name='flatten'),\n",
        "    layers.Dense(64, activation='relu', name='dense1'),\n",
        "    layers.Dense(10, activation='softmax', name='output')\n",
        "], name='CIFAR10_CNN')\n",
        "\n",
        "# Resumen del modelo\n",
        "model.summary()\n",
        "\n",
        "# CompilaciÃ³n del modelo\n",
        "print(\"\\nâš™ï¸ Compilando modelo...\")\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"âœ… Modelo compilado correctamente!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5ï¸âƒ£ Entrenamiento del Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ConfiguraciÃ³n del entrenamiento\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "VALIDATION_SPLIT = 0.1\n",
        "\n",
        "print(f\"ğŸš€ Iniciando entrenamiento...\")\n",
        "print(f\"   - Ã‰pocas: {EPOCHS}\")\n",
        "print(f\"   - Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   - ValidaciÃ³n: {VALIDATION_SPLIT*100}%\")\n",
        "print(f\"\\nInicio: {datetime.now().strftime('%H:%M:%S')}\\n\")\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(\n",
        "    train_images_norm,\n",
        "    train_labels_cat,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=VALIDATION_SPLIT,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Entrenamiento completado!\")\n",
        "print(f\"Fin: {datetime.now().strftime('%H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6ï¸âƒ£ EvaluaciÃ³n y VisualizaciÃ³n de Resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluar el modelo en el conjunto de prueba\n",
        "print(\"ğŸ§ª Evaluando modelo en conjunto de prueba...\\n\")\n",
        "test_loss, test_accuracy = model.evaluate(test_images_norm, test_labels_cat, verbose=1)\n",
        "\n",
        "print(f\"\\nğŸ“Š Resultados Finales:\")\n",
        "print(f\"   - PÃ©rdida en Test: {test_loss:.4f}\")\n",
        "print(f\"   - PrecisiÃ³n en Test: {test_accuracy*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GrÃ¡ficos de evoluciÃ³n del entrenamiento\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# GrÃ¡fico de PrecisiÃ³n\n",
        "ax1.plot(history.history['accuracy'], 'b-o', label='Entrenamiento', linewidth=2, markersize=6)\n",
        "ax1.plot(history.history['val_accuracy'], 'r-s', label='ValidaciÃ³n', linewidth=2, markersize=6)\n",
        "ax1.set_title('ğŸ“ˆ EvoluciÃ³n de la PrecisiÃ³n', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Ã‰poca', fontsize=12)\n",
        "ax1.set_ylabel('PrecisiÃ³n', fontsize=12)\n",
        "ax1.legend(fontsize=11)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_ylim([0, 1])\n",
        "\n",
        "# GrÃ¡fico de PÃ©rdida\n",
        "ax2.plot(history.history['loss'], 'b-o', label='Entrenamiento', linewidth=2, markersize=6)\n",
        "ax2.plot(history.history['val_loss'], 'r-s', label='ValidaciÃ³n', linewidth=2, markersize=6)\n",
        "ax2.set_title('ğŸ“‰ EvoluciÃ³n de la PÃ©rdida', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Ã‰poca', fontsize=12)\n",
        "ax2.set_ylabel('PÃ©rdida', fontsize=12)\n",
        "ax2.legend(fontsize=11)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7ï¸âƒ£ Predicciones y Pruebas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PredicciÃ³n detallada de una imagen aleatoria\n",
        "idx = np.random.randint(0, len(test_images))\n",
        "image = test_images[idx]\n",
        "true_label = test_labels[idx][0]\n",
        "\n",
        "# Predecir\n",
        "pred_probs = model.predict(test_images_norm[idx:idx+1], verbose=0)[0]\n",
        "pred_label = np.argmax(pred_probs)\n",
        "\n",
        "# VisualizaciÃ³n\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Imagen\n",
        "ax1.imshow(image)\n",
        "ax1.set_title(f\"Imagen #{idx}\\nClase Real: {class_names[true_label]}\", \n",
        "             fontsize=12, fontweight='bold')\n",
        "ax1.axis('off')\n",
        "\n",
        "# Probabilidades\n",
        "colors = ['green' if i == pred_label else 'skyblue' for i in range(10)]\n",
        "bars = ax2.barh(class_names, pred_probs * 100, color=colors)\n",
        "ax2.set_xlabel('Probabilidad (%)', fontsize=11, fontweight='bold')\n",
        "ax2.set_title(f'PredicciÃ³n: {class_names[pred_label]} ({pred_probs[pred_label]*100:.1f}%)',\n",
        "             fontsize=12, fontweight='bold', \n",
        "             color='green' if pred_label == true_label else 'red')\n",
        "ax2.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# AÃ±adir valores\n",
        "for i, (bar, prob) in enumerate(zip(bars, pred_probs * 100)):\n",
        "    if prob > 2:\n",
        "        ax2.text(prob + 1, i, f'{prob:.1f}%', va='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Resultado\n",
        "if pred_label == true_label:\n",
        "    print(\"âœ… Â¡PredicciÃ³n CORRECTA!\")\n",
        "else:\n",
        "    print(f\"âŒ PredicciÃ³n INCORRECTA. Se predijo '{class_names[pred_label]}' pero era '{class_names[true_label]}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8ï¸âƒ£ Guardar el Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar el modelo entrenado\n",
        "model_filename = 'cifar10_cnn_model.h5'\n",
        "model.save(model_filename)\n",
        "print(f\"ğŸ’¾ Modelo guardado como: {model_filename}\")\n",
        "\n",
        "# TamaÃ±o del modelo\n",
        "import os\n",
        "if os.path.exists(model_filename):\n",
        "    model_size = os.path.getsize(model_filename) / (1024 * 1024)\n",
        "    print(f\"ğŸ“¦ TamaÃ±o del modelo: {model_size:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Descargar el modelo (funciona solo en Google Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(model_filename)\n",
        "    print(f\"â¬‡ï¸ Descargando modelo...\")\n",
        "except:\n",
        "    print(\"â„¹ï¸ No estÃ¡s en Google Colab. El modelo se ha guardado localmente.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“Š Resumen del Proyecto\n",
        "\n",
        "### ğŸ¯ Objetivos Alcanzados:\n",
        "- âœ… Carga y preprocesamiento del dataset CIFAR-10\n",
        "- âœ… ConstrucciÃ³n de arquitectura CNN con 2 bloques convolucionales\n",
        "- âœ… Entrenamiento del modelo con validaciÃ³n\n",
        "- âœ… EvaluaciÃ³n y anÃ¡lisis de resultados\n",
        "- âœ… VisualizaciÃ³n de predicciones\n",
        "- âœ… Guardado del modelo entrenado\n",
        "\n",
        "### ğŸ“ˆ MÃ©tricas Esperadas:\n",
        "- **PrecisiÃ³n en Test**: ~70-75%\n",
        "- **Total de parÃ¡metros**: ~167.562\n",
        "- **Tiempo de entrenamiento**: ~5-10 minutos (10 Ã©pocas con GPU)\n",
        "\n",
        "### ğŸš€ PrÃ³ximos Pasos:\n",
        "1. Experimentar con mÃ¡s Ã©pocas de entrenamiento\n",
        "2. AÃ±adir capas de Dropout para reducir overfitting\n",
        "3. Probar Data Augmentation para mejorar generalizaciÃ³n\n",
        "4. Implementar arquitecturas mÃ¡s complejas (ResNet, VGG)\n",
        "\n",
        "### ğŸ”— Enlaces Ãštiles:\n",
        "- [Repositorio GitHub](https://github.com/Alaedddine718/La_Vision_Artificial)\n",
        "- [Dataset CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
        "- [DocumentaciÃ³n TensorFlow](https://www.tensorflow.org/)\n",
        "\n",
        "---\n",
        "\n",
        "**Desarrollado por:**  \n",
        "ğŸ‘¨â€ğŸ’» Alessio Cicilano  \n",
        "ğŸ‘¨â€ğŸ’» Alaeddine Daoudi\n",
        "\n",
        "**Octubre 2025**\n",
        "\n",
        "ğŸ Python | ğŸ§  TensorFlow/Keras | ğŸ“Š Deep Learning | ğŸ–¼ï¸ Computer Vision\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
