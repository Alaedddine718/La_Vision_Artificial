{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üñºÔ∏è Clasificador CNN para CIFAR-10\n",
        "\n",
        "**Autores:** Alessio Cicilano & Alaeddine Daoudi  \n",
        "**Fecha:** Octubre 2025\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Descripci√≥n\n",
        "\n",
        "Este notebook implementa una **Red Neuronal Convolucional (CNN)** para clasificar im√°genes del dataset CIFAR-10 en 10 categor√≠as:\n",
        "\n",
        "‚úàÔ∏è Avi√≥n | üöó Autom√≥vil | üê¶ P√°jaro | üê± Gato | ü¶å Ciervo | üêï Perro | üê∏ Rana | üê¥ Caballo | ‚õµ Barco | üöö Cami√≥n\n",
        "\n",
        "---\n",
        "\n",
        "### üèóÔ∏è Arquitectura del Modelo\n",
        "\n",
        "- **Conv2D** (32 filtros 3x3) + ReLU + MaxPooling\n",
        "- **Conv2D** (64 filtros 3x3) + ReLU + MaxPooling\n",
        "- **Flatten**\n",
        "- **Dense** (64 neuronas) + ReLU\n",
        "- **Dense** (10 neuronas) + Softmax\n",
        "\n",
        "**Total par√°metros:** ~167.562\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Configuraci√≥n e Importaci√≥n de Librer√≠as\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar librer√≠as necesarias\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Configuraci√≥n de estilo\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
        "print(f\"‚úÖ GPU disponible: {tf.config.list_physical_devices('GPU')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ Carga y Preprocesamiento del Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar dataset CIFAR-10\n",
        "print(\"üì• Cargando dataset CIFAR-10...\")\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "# Nombres de las clases\n",
        "class_names = ['Avi√≥n', 'Autom√≥vil', 'P√°jaro', 'Gato', 'Ciervo', \n",
        "               'Perro', 'Rana', 'Caballo', 'Barco', 'Cami√≥n']\n",
        "\n",
        "# Informaci√≥n del dataset\n",
        "print(f\"\\nüìä Informaci√≥n del Dataset:\")\n",
        "print(f\"   - Im√°genes de entrenamiento: {train_images.shape[0]:,}\")\n",
        "print(f\"   - Im√°genes de prueba: {test_images.shape[0]:,}\")\n",
        "print(f\"   - Dimensiones de imagen: {train_images.shape[1:]}\")\n",
        "print(f\"   - N√∫mero de clases: {len(class_names)}\")\n",
        "print(f\"   - Rango de p√≠xeles: [{train_images.min()}, {train_images.max()}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalizaci√≥n de las im√°genes (0-255 -> 0-1)\n",
        "print(\"üîß Normalizando im√°genes...\")\n",
        "train_images_norm = train_images.astype('float32') / 255.0\n",
        "test_images_norm = test_images.astype('float32') / 255.0\n",
        "\n",
        "# Convertir etiquetas a one-hot encoding\n",
        "print(\"üîß Codificando etiquetas...\")\n",
        "train_labels_cat = to_categorical(train_labels, 10)\n",
        "test_labels_cat = to_categorical(test_labels, 10)\n",
        "\n",
        "print(f\"\\n‚úÖ Preprocesamiento completado!\")\n",
        "print(f\"   - Rango normalizado: [{train_images_norm.min()}, {train_images_norm.max()}]\")\n",
        "print(f\"   - Shape etiquetas: {train_labels_cat.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Visualizaci√≥n del Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar 25 im√°genes aleatorias del dataset\n",
        "plt.figure(figsize=(12, 12))\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    idx = np.random.randint(0, len(train_images))\n",
        "    plt.imshow(train_images[idx])\n",
        "    plt.title(f\"{class_names[train_labels[idx][0]]}\", fontsize=10, fontweight='bold')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.suptitle('üñºÔ∏è Ejemplos del Dataset CIFAR-10', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Construcci√≥n del Modelo CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construcci√≥n de la arquitectura CNN\n",
        "print(\"üèóÔ∏è Construyendo modelo CNN...\\n\")\n",
        "\n",
        "model = models.Sequential([\n",
        "    # Primer bloque convolucional\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3), name='conv1'),\n",
        "    layers.MaxPooling2D((2, 2), name='pool1'),\n",
        "    \n",
        "    # Segundo bloque convolucional\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', name='conv2'),\n",
        "    layers.MaxPooling2D((2, 2), name='pool2'),\n",
        "    \n",
        "    # Clasificador\n",
        "    layers.Flatten(name='flatten'),\n",
        "    layers.Dense(64, activation='relu', name='dense1'),\n",
        "    layers.Dense(10, activation='softmax', name='output')\n",
        "], name='CIFAR10_CNN')\n",
        "\n",
        "# Resumen del modelo\n",
        "model.summary()\n",
        "\n",
        "# Compilaci√≥n del modelo\n",
        "print(\"\\n‚öôÔ∏è Compilando modelo...\")\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelo compilado correctamente!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ Entrenamiento del Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuraci√≥n del entrenamiento\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "VALIDATION_SPLIT = 0.1\n",
        "\n",
        "print(f\"üöÄ Iniciando entrenamiento...\")\n",
        "print(f\"   - √âpocas: {EPOCHS}\")\n",
        "print(f\"   - Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   - Validaci√≥n: {VALIDATION_SPLIT*100}%\")\n",
        "print(f\"\\nInicio: {datetime.now().strftime('%H:%M:%S')}\\n\")\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(\n",
        "    train_images_norm,\n",
        "    train_labels_cat,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=VALIDATION_SPLIT,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Entrenamiento completado!\")\n",
        "print(f\"Fin: {datetime.now().strftime('%H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6Ô∏è‚É£ Evaluaci√≥n y Visualizaci√≥n de Resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluar el modelo en el conjunto de prueba\n",
        "print(\"üß™ Evaluando modelo en conjunto de prueba...\\n\")\n",
        "test_loss, test_accuracy = model.evaluate(test_images_norm, test_labels_cat, verbose=1)\n",
        "\n",
        "print(f\"\\nüìä Resultados Finales:\")\n",
        "print(f\"   - P√©rdida en Test: {test_loss:.4f}\")\n",
        "print(f\"   - Precisi√≥n en Test: {test_accuracy*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gr√°ficos de evoluci√≥n del entrenamiento\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Gr√°fico de Precisi√≥n\n",
        "ax1.plot(history.history['accuracy'], 'b-o', label='Entrenamiento', linewidth=2, markersize=6)\n",
        "ax1.plot(history.history['val_accuracy'], 'r-s', label='Validaci√≥n', linewidth=2, markersize=6)\n",
        "ax1.set_title('üìà Evoluci√≥n de la Precisi√≥n', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('√âpoca', fontsize=12)\n",
        "ax1.set_ylabel('Precisi√≥n', fontsize=12)\n",
        "ax1.legend(fontsize=11)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_ylim([0, 1])\n",
        "\n",
        "# Gr√°fico de P√©rdida\n",
        "ax2.plot(history.history['loss'], 'b-o', label='Entrenamiento', linewidth=2, markersize=6)\n",
        "ax2.plot(history.history['val_loss'], 'r-s', label='Validaci√≥n', linewidth=2, markersize=6)\n",
        "ax2.set_title('üìâ Evoluci√≥n de la P√©rdida', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('√âpoca', fontsize=12)\n",
        "ax2.set_ylabel('P√©rdida', fontsize=12)\n",
        "ax2.legend(fontsize=11)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7Ô∏è‚É£ Predicciones y Pruebas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predicci√≥n detallada de una imagen aleatoria\n",
        "idx = np.random.randint(0, len(test_images))\n",
        "image = test_images[idx]\n",
        "true_label = test_labels[idx][0]\n",
        "\n",
        "# Predecir\n",
        "pred_probs = model.predict(test_images_norm[idx:idx+1], verbose=0)[0]\n",
        "pred_label = np.argmax(pred_probs)\n",
        "\n",
        "# Visualizaci√≥n\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Imagen\n",
        "ax1.imshow(image)\n",
        "ax1.set_title(f\"Imagen #{idx}\\nClase Real: {class_names[true_label]}\", \n",
        "             fontsize=12, fontweight='bold')\n",
        "ax1.axis('off')\n",
        "\n",
        "# Probabilidades\n",
        "colors = ['green' if i == pred_label else 'skyblue' for i in range(10)]\n",
        "bars = ax2.barh(class_names, pred_probs * 100, color=colors)\n",
        "ax2.set_xlabel('Probabilidad (%)', fontsize=11, fontweight='bold')\n",
        "ax2.set_title(f'Predicci√≥n: {class_names[pred_label]} ({pred_probs[pred_label]*100:.1f}%)',\n",
        "             fontsize=12, fontweight='bold', \n",
        "             color='green' if pred_label == true_label else 'red')\n",
        "ax2.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# A√±adir valores\n",
        "for i, (bar, prob) in enumerate(zip(bars, pred_probs * 100)):\n",
        "    if prob > 2:\n",
        "        ax2.text(prob + 1, i, f'{prob:.1f}%', va='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Resultado\n",
        "if pred_label == true_label:\n",
        "    print(\"‚úÖ ¬°Predicci√≥n CORRECTA!\")\n",
        "else:\n",
        "    print(f\"‚ùå Predicci√≥n INCORRECTA. Se predijo '{class_names[pred_label]}' pero era '{class_names[true_label]}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8Ô∏è‚É£ Guardar el Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar el modelo entrenado\n",
        "model_filename = 'cifar10_cnn_model.h5'\n",
        "model.save(model_filename)\n",
        "print(f\"üíæ Modelo guardado como: {model_filename}\")\n",
        "\n",
        "# Tama√±o del modelo\n",
        "import os\n",
        "if os.path.exists(model_filename):\n",
        "    model_size = os.path.getsize(model_filename) / (1024 * 1024)\n",
        "    print(f\"üì¶ Tama√±o del modelo: {model_size:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Descargar el modelo (funciona solo en Google Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(model_filename)\n",
        "    print(f\"‚¨áÔ∏è Descargando modelo...\")\n",
        "except:\n",
        "    print(\"‚ÑπÔ∏è No est√°s en Google Colab. El modelo se ha guardado localmente.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Resumen del Proyecto\n",
        "\n",
        "### üéØ Objetivos Alcanzados:\n",
        "- ‚úÖ Carga y preprocesamiento del dataset CIFAR-10\n",
        "- ‚úÖ Construcci√≥n de arquitectura CNN con 2 bloques convolucionales\n",
        "- ‚úÖ Entrenamiento del modelo con validaci√≥n\n",
        "- ‚úÖ Evaluaci√≥n y an√°lisis de resultados\n",
        "- ‚úÖ Visualizaci√≥n de predicciones\n",
        "- ‚úÖ Guardado del modelo entrenado\n",
        "\n",
        "### üìà M√©tricas Esperadas:\n",
        "- **Precisi√≥n en Test**: ~70-75%\n",
        "- **Total de par√°metros**: ~167.562\n",
        "- **Tiempo de entrenamiento**: ~5-10 minutos (10 √©pocas con GPU)\n",
        "\n",
        "### üöÄ Pr√≥ximos Pasos:\n",
        "1. Experimentar con m√°s √©pocas de entrenamiento\n",
        "2. A√±adir capas de Dropout para reducir overfitting\n",
        "3. Probar Data Augmentation para mejorar generalizaci√≥n\n",
        "4. Implementar arquitecturas m√°s complejas (ResNet, VGG)\n",
        "\n",
        "### üîó Enlaces √ötiles:\n",
        "- [Repositorio GitHub](https://github.com/Alaedddine718/La_Vision_Artificial)\n",
        "- [Dataset CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
        "- [Documentaci√≥n TensorFlow](https://www.tensorflow.org/)\n",
        "\n",
        "---\n",
        "\n",
        "**Desarrollado por:**  \n",
        "üë®‚Äçüíª Alessio Cicilano  \n",
        "üë®‚Äçüíª Alaeddine Daoudi\n",
        "\n",
        "**Octubre 2025**\n",
        "\n",
        "üêç Python | üß† TensorFlow/Keras | üìä Deep Learning | üñºÔ∏è Computer Vision\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
